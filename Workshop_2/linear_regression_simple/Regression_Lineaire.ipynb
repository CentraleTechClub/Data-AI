{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ebfca5",
   "metadata": {},
   "source": [
    "# R√©gression Lin√©aire De Z√©ro\n",
    "\n",
    "Dans ce notebook, vous allez apprendre √† impl√©menter la **R√©gression Lin√©aire** de z√©ro en utilisant uniquement NumPy. \n",
    "\n",
    "## Objectifs d'Apprentissage\n",
    "- Comprendre les fondements math√©matiques de la r√©gression lin√©aire\n",
    "- Impl√©menter la fonction de pr√©diction : $y = wx + b$\n",
    "- Calculer l'erreur quadratique moyenne (MSE)\n",
    "- Calculer les gradients en utilisant le calcul diff√©rentiel\n",
    "- Impl√©menter l'optimisation par descente de gradient\n",
    "- Entra√Æner un mod√®le pour pr√©dire les notes des √©tudiants en fonction du temps d'√©tude\n",
    "- Sauvegarder et charger les poids du mod√®le entra√Æn√©\n",
    "- Faire des pr√©dictions sur de nouvelles donn√©es\n",
    "\n",
    "## Jeu de Donn√©es\n",
    "Nous utiliserons un jeu de donn√©es sur les temps d'√©tude des √©tudiants (en minutes) et leurs notes correspondantes (0-100)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4912a1",
   "metadata": {},
   "source": [
    "## Table des Mati√®res\n",
    "- [1 - Importer les Packages](#1)\n",
    "- [2 - Charger et Visualiser le Jeu de Donn√©es](#2)\n",
    "- [3 - Th√©orie de la R√©gression Lin√©aire](#3)\n",
    "  - [3.1 - La Formule de Pr√©diction](#3-1)\n",
    "  - [3.2 - Erreur Quadratique Moyenne (MSE)](#3-2)\n",
    "  - [3.3 - Calcul des Gradients](#3-3)\n",
    "  - [3.4 - Descente de Gradient](#3-4)\n",
    "- [4 - Impl√©mentation De Z√©ro](#4)\n",
    "  - [4.1 - Fonction de Pr√©diction](#4-1)\n",
    "  - [4.2 - Fonction de Perte MSE](#4-2)\n",
    "  - [4.3 - Calcul des Gradients](#4-3)\n",
    "  - [4.4 - Descente de Gradient](#4-4)\n",
    "  - [4.5 - Fonction d'Entra√Ænement](#4-5)\n",
    "- [5 - Entra√Æner le Mod√®le](#5)\n",
    "- [6 - Sauvegarder les Poids du Mod√®le](#6)\n",
    "- [7 - Charger les Poids du Mod√®le](#7)\n",
    "- [8 - Tester sur Nouvelles Donn√©es](#8)\n",
    "- [9 - Visualiser les R√©sultats](#9)\n",
    "- [10 - Conclusion](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3fff4",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Importer les Packages\n",
    "\n",
    "Commen√ßons par importer les packages n√©cessaires :\n",
    "- **numpy** : Pour les calculs num√©riques\n",
    "- **matplotlib** : Pour les visualisations\n",
    "- **pandas** : Pour charger et manipuler le jeu de donn√©es\n",
    "- **pickle** : Pour sauvegarder et charger les poids du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da5c489",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Charger et Visualiser le Jeu de Donn√©es\n",
    "\n",
    "Chargeons notre jeu de donn√©es contenant les temps d'√©tude des √©tudiants et leurs notes correspondantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a10c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le jeu de donn√©es\n",
    "data = pd.read_csv('study_grades_dataset.csv')\n",
    "\n",
    "# Afficher les premi√®res lignes\n",
    "print(\"Premiers 5 √©chantillons du jeu de donn√©es :\")\n",
    "print(data.head())\n",
    "\n",
    "# Afficher les statistiques du jeu de donn√©es\n",
    "print(\"\\nStatistiques du Jeu de Donn√©es :\")\n",
    "print(data.describe())\n",
    "\n",
    "print(f\"\\nNombre total d'√©chantillons : {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les caract√©ristiques (X) et les √©tiquettes (y)\n",
    "X = data['study_time_minutes'].values\n",
    "y = data['grade'].values\n",
    "\n",
    "print(f\"Forme de X : {X.shape}\")\n",
    "print(f\"Forme de y : {y.shape}\")\n",
    "\n",
    "# Visualiser le jeu de donn√©es\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, color='blue', edgecolors='k')\n",
    "plt.xlabel('Temps d\\'√âtude (minutes)', fontsize=12)\n",
    "plt.ylabel('Note', fontsize=12)\n",
    "plt.title('Notes des √âtudiants vs Temps d\\'√âtude', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComme vous pouvez le constater, il y a une corr√©lation positive entre le temps d'√©tude et les notes !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6388f30d",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Th√©orie de la R√©gression Lin√©aire\n",
    "\n",
    "Avant d'impl√©menter, comprenons les math√©matiques derri√®re la r√©gression lin√©aire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dfc8da",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - La Formule de Pr√©diction\n",
    "\n",
    "En r√©gression lin√©aire, nous voulons trouver une relation lin√©aire entre la caract√©ristique d'entr√©e $x$ (temps d'√©tude) et la sortie $y$ (note).\n",
    "\n",
    "La formule de pr√©diction est :\n",
    "\n",
    "$$\\hat{y} = wx + b$$\n",
    "\n",
    "O√π :\n",
    "- $\\hat{y}$ est la valeur pr√©dite\n",
    "- $x$ est la caract√©ristique d'entr√©e (temps d'√©tude)\n",
    "- $w$ est le poids (pente de la droite)\n",
    "- $b$ est le biais (ordonn√©e √† l'origine)\n",
    "\n",
    "Pour plusieurs √©chantillons, nous pouvons vectoriser ceci comme :\n",
    "\n",
    "$$\\hat{Y} = wX + b$$\n",
    "\n",
    "O√π $X$ et $\\hat{Y}$ sont des vecteurs de toutes les caract√©ristiques d'entr√©e et pr√©dictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e6515",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Erreur Quadratique Moyenne (MSE)\n",
    "\n",
    "Pour entra√Æner notre mod√®le, nous devons mesurer √† quel point nos pr√©dictions sont √©loign√©es des valeurs r√©elles. Nous utilisons **l'Erreur Quadratique Moyenne (MSE)** comme fonction de perte :\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "O√π :\n",
    "- $m$ est le nombre d'exemples d'entra√Ænement\n",
    "- $\\hat{y}^{(i)}$ est la valeur pr√©dite pour l'exemple $i$\n",
    "- $y^{(i)}$ est la valeur r√©elle pour l'exemple $i$\n",
    "\n",
    "La MSE mesure la diff√©rence quadratique moyenne entre les pr√©dictions et les valeurs r√©elles. Notre objectif est de **minimiser cette erreur** en trouvant les meilleures valeurs pour $w$ et $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56f594",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Calcul des Gradients\n",
    "\n",
    "Pour minimiser la MSE, nous devons savoir comment la perte change lorsque nous ajustons $w$ et $b$. Cela se fait en utilisant les **gradients** (d√©riv√©es partielles).\n",
    "\n",
    "Le gradient de la MSE par rapport √† $w$ :\n",
    "\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial w} = \\frac{2}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) \\cdot x^{(i)}$$\n",
    "\n",
    "Sous forme vectoris√©e :\n",
    "\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial w} = \\frac{2}{m} X^T (\\hat{Y} - Y)$$\n",
    "\n",
    "Le gradient de la MSE par rapport √† $b$ :\n",
    "\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial b} = \\frac{2}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})$$\n",
    "\n",
    "Sous forme vectoris√©e :\n",
    "\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial b} = \\frac{2}{m} \\sum (\\hat{Y} - Y)$$\n",
    "\n",
    "Ces gradients nous indiquent la direction et l'amplitude pour ajuster $w$ et $b$ afin de r√©duire l'erreur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ddf1d",
   "metadata": {},
   "source": [
    "<a name='3-4'></a>\n",
    "### 3.4 - Descente de Gradient\n",
    "\n",
    "La **Descente de Gradient** est un algorithme d'optimisation qui ajuste it√©rativement les param√®tres pour minimiser la fonction de perte.\n",
    "\n",
    "Les r√®gles de mise √† jour sont :\n",
    "\n",
    "$$w := w - \\alpha \\frac{\\partial \\text{MSE}}{\\partial w}$$\n",
    "\n",
    "$$b := b - \\alpha \\frac{\\partial \\text{MSE}}{\\partial b}$$\n",
    "\n",
    "O√π :\n",
    "- $\\alpha$ est le **taux d'apprentissage** (contr√¥le la taille du pas)\n",
    "- Les gradients nous indiquent dans quelle direction nous d√©placer\n",
    "\n",
    "Nous r√©p√©tons ce processus pendant de nombreuses it√©rations jusqu'√† ce que le mod√®le converge (la perte cesse de diminuer significativement)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4cf7a8",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Impl√©mentation De Z√©ro\n",
    "\n",
    "Maintenant impl√©mentons chaque composant de la r√©gression lin√©aire de z√©ro !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90e120",
   "metadata": {},
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 - Fonction de Pr√©diction\n",
    "\n",
    "D'abord, impl√©mentons la fonction de pr√©diction : $\\hat{y} = wx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1efda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Calculer les pr√©dictions en utilisant la formule de r√©gression lin√©aire.\n",
    "    \n",
    "    Arguments :\n",
    "    X -- caract√©ristiques d'entr√©e, tableau numpy de forme (m,) o√π m est le nombre d'exemples\n",
    "    w -- param√®tre de poids (pente)\n",
    "    b -- param√®tre de biais (ordonn√©e √† l'origine)\n",
    "    \n",
    "    Retourne :\n",
    "    y_pred -- pr√©dictions, tableau numpy de forme (m,)\n",
    "    \"\"\"\n",
    "    y_pred = w * X + b\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la fonction de pr√©diction\n",
    "test_X = np.array([100, 200, 300])\n",
    "test_w = 0.03\n",
    "test_b = 40\n",
    "\n",
    "test_predictions = predict(test_X, test_w, test_b)\n",
    "print(f\"Entr√©es de test : {test_X}\")\n",
    "print(f\"Pr√©dictions de test : {test_predictions}\")\n",
    "print(f\"\\nAvec w={test_w} et b={test_b} :\")\n",
    "print(f\"  - {test_X[0]} minutes d'√©tude ‚Üí note pr√©dite : {test_predictions[0]}\")\n",
    "print(f\"  - {test_X[1]} minutes d'√©tude ‚Üí note pr√©dite : {test_predictions[1]}\")\n",
    "print(f\"  - {test_X[2]} minutes d'√©tude ‚Üí note pr√©dite : {test_predictions[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7a57d",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Fonction de Perte MSE\n",
    "\n",
    "Maintenant impl√©mentons la fonction de perte d'Erreur Quadratique Moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b23114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculer l'Erreur Quadratique Moyenne entre les valeurs vraies et pr√©dites.\n",
    "    \n",
    "    Arguments :\n",
    "    y_true -- valeurs r√©elles, tableau numpy de forme (m,)\n",
    "    y_pred -- valeurs pr√©dites, tableau numpy de forme (m,)\n",
    "    \n",
    "    Retourne :\n",
    "    mse -- erreur quadratique moyenne (scalaire)\n",
    "    \"\"\"\n",
    "    m = len(y_true)\n",
    "    mse = (1/m) * np.sum((y_pred - y_true)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la fonction MSE\n",
    "test_y_true = np.array([50, 60, 70])\n",
    "test_y_pred = np.array([48, 62, 68])\n",
    "\n",
    "test_mse = compute_mse(test_y_true, test_y_pred)\n",
    "print(f\"Valeurs vraies : {test_y_true}\")\n",
    "print(f\"Valeurs pr√©dites : {test_y_pred}\")\n",
    "print(f\"Erreur Quadratique Moyenne : {test_mse:.2f}\")\n",
    "print(f\"\\nUne MSE plus faible signifie de meilleures pr√©dictions !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dc124",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Calcul des Gradients\n",
    "\n",
    "Impl√©menter le calcul du gradient pour $w$ et $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b903d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculer les gradients de la MSE par rapport √† w et b.\n",
    "    \n",
    "    Arguments :\n",
    "    X -- caract√©ristiques d'entr√©e, tableau numpy de forme (m,)\n",
    "    y_true -- valeurs r√©elles, tableau numpy de forme (m,)\n",
    "    y_pred -- valeurs pr√©dites, tableau numpy de forme (m,)\n",
    "    \n",
    "    Retourne :\n",
    "    dw -- gradient par rapport √† w (scalaire)\n",
    "    db -- gradient par rapport √† b (scalaire)\n",
    "    \"\"\"\n",
    "    m = len(y_true)\n",
    "    \n",
    "    # Calculer l'erreur\n",
    "    error = y_pred - y_true\n",
    "    \n",
    "    # Calculer les gradients\n",
    "    dw = (2/m) * np.sum(error * X)\n",
    "    db = (2/m) * np.sum(error)\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la fonction de gradient\n",
    "test_X = np.array([100, 200, 300])\n",
    "test_y_true = np.array([50, 60, 70])\n",
    "test_y_pred = np.array([48, 62, 68])\n",
    "\n",
    "test_dw, test_db = compute_gradients(test_X, test_y_true, test_y_pred)\n",
    "print(f\"Gradient par rapport √† w (dw) : {test_dw:.4f}\")\n",
    "print(f\"Gradient par rapport √† b (db) : {test_db:.4f}\")\n",
    "print(f\"\\nCes gradients nous indiquent comment ajuster w et b pour r√©duire l'erreur.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f1a0f",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Descente de Gradient\n",
    "\n",
    "Impl√©menter une √©tape de descente de gradient pour mettre √† jour les param√®tres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(w, b, dw, db, learning_rate):\n",
    "    \"\"\"\n",
    "    Effectuer une √©tape de descente de gradient.\n",
    "    \n",
    "    Arguments :\n",
    "    w -- poids actuel\n",
    "    b -- biais actuel\n",
    "    dw -- gradient par rapport √† w\n",
    "    db -- gradient par rapport √† b\n",
    "    learning_rate -- taille du pas pour les mises √† jour\n",
    "    \n",
    "    Retourne :\n",
    "    w_new -- poids mis √† jour\n",
    "    b_new -- biais mis √† jour\n",
    "    \"\"\"\n",
    "    w_new = w - learning_rate * dw\n",
    "    b_new = b - learning_rate * db\n",
    "    \n",
    "    return w_new, b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester l'√©tape de descente de gradient\n",
    "test_w = 0.03\n",
    "test_b = 40\n",
    "test_dw = 0.5\n",
    "test_db = 2.0\n",
    "test_lr = 0.01\n",
    "\n",
    "new_w, new_b = gradient_descent_step(test_w, test_b, test_dw, test_db, test_lr)\n",
    "print(f\"Avant mise √† jour : w = {test_w}, b = {test_b}\")\n",
    "print(f\"Gradients : dw = {test_dw}, db = {test_db}\")\n",
    "print(f\"Taux d'apprentissage : {test_lr}\")\n",
    "print(f\"Apr√®s mise √† jour : w = {new_w}, b = {new_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27881f4",
   "metadata": {},
   "source": [
    "<a name='4-5'></a>\n",
    "### 4.5 - Fonction d'Entra√Ænement\n",
    "\n",
    "Maintenant combinons tout dans une fonction d'entra√Ænement compl√®te."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(X, y, learning_rate=0.0001, num_iterations=1000, print_cost=True):\n",
    "    \"\"\"\n",
    "    Entra√Æner un mod√®le de r√©gression lin√©aire en utilisant la descente de gradient.\n",
    "    \n",
    "    Arguments :\n",
    "    X -- caract√©ristiques d'entr√©e, tableau numpy de forme (m,)\n",
    "    y -- valeurs cibles, tableau numpy de forme (m,)\n",
    "    learning_rate -- taux d'apprentissage pour la descente de gradient\n",
    "    num_iterations -- nombre d'it√©rations d'entra√Ænement\n",
    "    print_cost -- si True, afficher le co√ªt toutes les 100 it√©rations\n",
    "    \n",
    "    Retourne :\n",
    "    w -- poids entra√Æn√©\n",
    "    b -- biais entra√Æn√©\n",
    "    costs -- liste des co√ªts pendant l'entra√Ænement (pour le tra√ßage)\n",
    "    \"\"\"\n",
    "    # Initialiser les param√®tres\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Passe avant : calculer les pr√©dictions\n",
    "        y_pred = predict(X, w, b)\n",
    "        \n",
    "        # Calculer la perte\n",
    "        cost = compute_mse(y, y_pred)\n",
    "        \n",
    "        # Calculer les gradients\n",
    "        dw, db = compute_gradients(X, y, y_pred)\n",
    "        \n",
    "        # Mettre √† jour les param√®tres\n",
    "        w, b = gradient_descent_step(w, b, dw, db, learning_rate)\n",
    "        \n",
    "        # Enregistrer le co√ªt\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(f\"It√©ration {i} : MSE = {cost:.2f}\")\n",
    "    \n",
    "    # Afficher les r√©sultats finaux\n",
    "    if print_cost:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Entra√Ænement Termin√© !\")\n",
    "        print(f\"MSE Finale : {cost:.2f}\")\n",
    "        print(f\"Param√®tres appris : w = {w:.6f}, b = {b:.6f}\")\n",
    "        print(f\"{'='*50}\")\n",
    "    \n",
    "    return w, b, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf50cab",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Entra√Æner le Mod√®le\n",
    "\n",
    "Entra√Ænons notre mod√®le de r√©gression lin√©aire sur le jeu de donn√©es temps d'√©tude et notes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45363bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner le mod√®le\n",
    "print(\"D√©but de l'entra√Ænement...\\n\")\n",
    "\n",
    "w_trained, b_trained, training_costs = train_linear_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.00001,\n",
    "    num_iterations=2000,\n",
    "    print_cost=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2889d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer la courbe d'entra√Ænement\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(0, len(training_costs)*100, 100), training_costs, linewidth=2, color='red')\n",
    "plt.xlabel('It√©ration', fontsize=12)\n",
    "plt.ylabel('Erreur Quadratique Moyenne (MSE)', fontsize=12)\n",
    "plt.title('Perte d\\'Entra√Ænement au Fil du Temps', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"La diminution de la perte montre que notre mod√®le apprend !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2777454",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Sauvegarder les Poids du Mod√®le\n",
    "\n",
    "Maintenant que nous avons entra√Æn√© notre mod√®le, sauvegardons les poids pour pouvoir les r√©utiliser plus tard sans r√©entra√Æner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb60b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le dossier model s'il n'existe pas\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "    print(\"Dossier 'model' cr√©√©.\")\n",
    "\n",
    "# Sauvegarder les poids entra√Æn√©s\n",
    "model_weights = {\n",
    "    'w': w_trained,\n",
    "    'b': b_trained\n",
    "}\n",
    "\n",
    "with open('model/linear_regression_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(model_weights, f)\n",
    "\n",
    "print(f\"\\nPoids du mod√®le sauvegard√©s avec succ√®s !\")\n",
    "print(f\"Fichier : model/linear_regression_weights.pkl\")\n",
    "print(f\"Poids : w = {w_trained:.6f}, b = {b_trained:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49970600",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Charger les Poids du Mod√®le\n",
    "\n",
    "D√©montrons comment charger les poids sauvegard√©s. C'est utile lorsque vous voulez faire des pr√©dictions sans r√©entra√Æner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les poids sauvegard√©s\n",
    "with open('model/linear_regression_weights.pkl', 'rb') as f:\n",
    "    loaded_weights = pickle.load(f)\n",
    "\n",
    "w_loaded = loaded_weights['w']\n",
    "b_loaded = loaded_weights['b']\n",
    "\n",
    "print(\"Poids du mod√®le charg√©s avec succ√®s !\")\n",
    "print(f\"Poids charg√©s : w = {w_loaded:.6f}, b = {b_loaded:.6f}\")\n",
    "\n",
    "# V√©rifier qu'ils correspondent aux poids entra√Æn√©s\n",
    "print(f\"\\nV√©rification :\")\n",
    "print(f\"  w correspond : {np.isclose(w_loaded, w_trained)}\")\n",
    "print(f\"  b correspond : {np.isclose(b_loaded, b_trained)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bbc6a",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8 - Tester sur Nouvelles Donn√©es\n",
    "\n",
    "Maintenant utilisons notre mod√®le entra√Æn√© pour faire des pr√©dictions sur de nouveaux temps d'√©tude non vus !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des temps d'√©tude de test al√©atoires\n",
    "np.random.seed(42)\n",
    "test_study_times = np.random.randint(5, 3000, size=10)\n",
    "test_study_times = np.sort(test_study_times)\n",
    "\n",
    "# Faire des pr√©dictions en utilisant les poids charg√©s\n",
    "test_predictions = predict(test_study_times, w_loaded, b_loaded)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"Pr√©dictions sur Nouvelles Donn√©es :\")\n",
    "print(\"=\" * 50)\n",
    "for study_time, grade in zip(test_study_times, test_predictions):\n",
    "    print(f\"Temps d'√âtude : {study_time:4d} minutes ‚Üí Note Pr√©dite : {grade:.1f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de pr√©diction interactive\n",
    "def predict_grade(study_minutes):\n",
    "    \"\"\"\n",
    "    Pr√©dire la note pour un temps d'√©tude donn√©.\n",
    "    \n",
    "    Arguments :\n",
    "    study_minutes -- temps d'√©tude en minutes\n",
    "    \n",
    "    Retourne :\n",
    "    predicted_grade -- note pr√©dite (limit√©e √† 0-100)\n",
    "    \"\"\"\n",
    "    grade = predict(np.array([study_minutes]), w_loaded, b_loaded)[0]\n",
    "    # Limiter √† la plage de notes valide\n",
    "    grade = np.clip(grade, 0, 100)\n",
    "    return grade\n",
    "\n",
    "# Essayer quelques exemples\n",
    "print(\"\\nPr√©dictions Interactives :\")\n",
    "print(\"=\" * 50)\n",
    "example_times = [60, 300, 600, 1200, 2400]\n",
    "for time in example_times:\n",
    "    predicted = predict_grade(time)\n",
    "    print(f\"Si vous √©tudiez {time:4d} minutes, note pr√©dite : {predicted:.1f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüí° Astuce : Vous pouvez modifier example_times pour tester vos propres dur√©es d'√©tude !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be581b",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "## 9 - Visualiser les R√©sultats\n",
    "\n",
    "Visualisons √† quel point notre mod√®le s'ajuste aux donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des pr√©dictions pour l'ensemble du jeu de donn√©es\n",
    "y_pred_all = predict(X, w_loaded, b_loaded)\n",
    "\n",
    "# Cr√©er la visualisation\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Tracer les points de donn√©es originaux\n",
    "plt.scatter(X, y, alpha=0.5, color='blue', label='Donn√©es R√©elles', edgecolors='k')\n",
    "\n",
    "# Tracer la droite de r√©gression\n",
    "plt.plot(X, y_pred_all, color='red', linewidth=2, label='Ajustement de R√©gression Lin√©aire')\n",
    "\n",
    "# Tracer les pr√©dictions de test\n",
    "plt.scatter(test_study_times, test_predictions, color='green', \n",
    "            s=100, marker='*', label='Pr√©dictions de Test', \n",
    "            edgecolors='black', linewidth=1.5)\n",
    "\n",
    "plt.xlabel('Temps d\\'√âtude (minutes)', fontsize=12)\n",
    "plt.ylabel('Note', fontsize=12)\n",
    "plt.title('R√©gression Lin√©aire : Temps d\\'√âtude vs Note', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLa ligne rouge montre notre mod√®le appris : Note = {w_loaded:.6f} √ó Temps_√âtude + {b_loaded:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a97270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les m√©triques de performance finales\n",
    "final_mse = compute_mse(y, y_pred_all)\n",
    "rmse = np.sqrt(final_mse)\n",
    "\n",
    "# Calculer le R-carr√© (coefficient de d√©termination)\n",
    "ss_res = np.sum((y - y_pred_all)**2)  # Somme des carr√©s des r√©sidus\n",
    "ss_tot = np.sum((y - np.mean(y))**2)  # Somme totale des carr√©s\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"M√©triques de Performance du Mod√®le :\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Erreur Quadratique Moyenne (MSE) : {final_mse:.2f}\")\n",
    "print(f\"Racine de l'Erreur Quadratique Moyenne (RMSE) : {rmse:.2f}\")\n",
    "print(f\"R-carr√© (R¬≤) : {r_squared:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìä R¬≤ = {r_squared:.4f} signifie que notre mod√®le explique {r_squared*100:.2f}% de la variance des donn√©es !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcaa1e0",
   "metadata": {},
   "source": [
    "<a name='10'></a>\n",
    "## 10 - Conclusion\n",
    "\n",
    "F√©licitations ! üéâ Vous avez impl√©ment√© avec succ√®s la r√©gression lin√©aire de z√©ro !\n",
    "\n",
    "### Ce que vous avez appris :\n",
    "\n",
    "1. **Fondements Math√©matiques :**\n",
    "   - Formule de pr√©diction : $\\hat{y} = wx + b$\n",
    "   - Fonction de perte Erreur Quadratique Moyenne (MSE)\n",
    "   - Calcul du gradient en utilisant le calcul diff√©rentiel\n",
    "   - Optimisation par descente de gradient\n",
    "\n",
    "2. **Comp√©tences d'Impl√©mentation :**\n",
    "   - Construit tous les composants de z√©ro en utilisant NumPy\n",
    "   - Impl√©ment√© la passe avant (pr√©dictions)\n",
    "   - Impl√©ment√© la passe arri√®re (gradients)\n",
    "   - Cr√©√© une boucle d'entra√Ænement compl√®te\n",
    "\n",
    "3. **Flux de Travail ML Pratique :**\n",
    "   - Charg√© et visualis√© les donn√©es\n",
    "   - Entra√Æn√© un mod√®le avec descente de gradient\n",
    "   - Sauvegard√© les poids du mod√®le pour r√©utilisation\n",
    "   - Charg√© les poids et fait des pr√©dictions\n",
    "   - √âvalu√© la performance du mod√®le\n",
    "\n",
    "### Insights Cl√©s :\n",
    "\n",
    "- La r√©gression lin√©aire trouve la meilleure ligne droite √† travers les donn√©es\n",
    "- La descente de gradient am√©liore it√©rativement le mod√®le en suivant le gradient n√©gatif\n",
    "- Le taux d'apprentissage contr√¥le la vitesse √† laquelle le mod√®le apprend\n",
    "- La MSE mesure √† quel point les pr√©dictions sont proches des valeurs r√©elles\n",
    "- Les poids du mod√®le peuvent √™tre sauvegard√©s et r√©utilis√©s sans r√©entra√Æner\n",
    "\n",
    "### Prochaines √âtapes :\n",
    "\n",
    "- Essayer diff√©rents taux d'apprentissage et voir comment ils affectent l'entra√Ænement\n",
    "- Exp√©rimenter avec diff√©rents nombres d'it√©rations\n",
    "- Ajouter la normalisation des donn√©es pour am√©liorer l'entra√Ænement\n",
    "- Essayer la r√©gression polynomiale pour les relations non lin√©aires\n",
    "- Impl√©menter la r√©gularisation pour √©viter le surapprentissage\n",
    "\n",
    "### Rappel :\n",
    "\n",
    "C'est la base pour comprendre des mod√®les plus complexes comme les r√©seaux de neurones ! Les concepts que vous avez appris ici (passe avant, calcul de perte, gradients, optimisation) sont les m√™mes utilis√©s en apprentissage profond.\n",
    "\n",
    "Continuez √† pratiquer ! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53348fd6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus : D√©rivation Math√©matique\n",
    "\n",
    "Pour ceux int√©ress√©s par les math√©matiques, voici comment nous d√©rivons les gradients :\n",
    "\n",
    "### Perte MSE :\n",
    "$$L = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "### Gradient par rapport √† w :\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{m} \\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)})^2 \\right]$$\n",
    "\n",
    "En utilisant la r√®gle de la cha√Æne :\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{2}{m} \\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)}) \\cdot x^{(i)}$$\n",
    "\n",
    "### Gradient par rapport √† b :\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[ \\frac{1}{m} \\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)})^2 \\right]$$\n",
    "\n",
    "En utilisant la r√®gle de la cha√Æne :\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{2}{m} \\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)})$$\n",
    "\n",
    "Ce sont les formules que nous avons impl√©ment√©es dans notre fonction `compute_gradients` !"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
